#!/usr/bin/env python3
"""
Update metadata information in existing JSONL files.

This script updates JSONL files generated by the old version of tokenize_audio.py
by adding the complete original_metadata dictionary extracted from the source tar files.
"""

import argparse
import json
import logging
import sys
import tarfile
from pathlib import Path

from tokenize_audio import load_audio_mono, audio_metadata_to_dict


def setup_logging(verbose: bool = False):
    """Configure logging to console."""
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )


def find_matching_jsonl_files(pattern: str) -> list[Path]:
    """
    Find JSONL files matching the given pattern.

    Args:
        pattern: Glob pattern for matching JSONL files

    Returns:
        List of Path objects for matching files
    """
    # If pattern is a directory, search within it
    path = Path(pattern)
    if path.is_dir():
        pattern = str(path / "*.jsonl")

    # Use the parent directory and pattern
    if "*" in pattern or "?" in pattern:
        pattern_path = Path(pattern)
        parent = pattern_path.parent
        glob_pattern = pattern_path.name
        files = sorted(parent.glob(glob_pattern))
    else:
        # Single file
        files = [Path(pattern)] if Path(pattern).exists() else []

    return files


def get_tar_path_for_jsonl(jsonl_path: Path, tar_source_dir: Path) -> Path:
    """
    Get the corresponding tar file path for a JSONL file.

    Args:
        jsonl_path: Path to the JSONL file
        tar_source_dir: Directory containing source tar files

    Returns:
        Path to the corresponding tar file
    """
    # Strip .jsonl extension and add .tar
    tar_filename = jsonl_path.stem + ".tar"
    tar_path = tar_source_dir / tar_filename
    return tar_path


def update_jsonl_with_metadata(
    jsonl_path: Path,
    tar_path: Path
) -> int:
    """
    Update a JSONL file with metadata from the corresponding tar file.

    Args:
        jsonl_path: Path to the JSONL file to update
        tar_path: Path to the source tar file

    Returns:
        Number of entries updated

    Raises:
        ValueError: If the order of entries doesn't match
        FileNotFoundError: If tar file doesn't exist
    """
    if not tar_path.exists():
        raise FileNotFoundError(f"Tar file not found: {tar_path}")

    logging.info(f"Processing: {jsonl_path}")
    logging.info(f"Source tar: {tar_path}")

    # Create temporary output file
    temp_path = jsonl_path.with_suffix('.jsonl.temp')

    # Process line-by-line alongside tar members
    with tarfile.open(tar_path, mode="r") as tar, \
         open(jsonl_path, 'r', encoding='utf-8') as jsonl_f, \
         open(temp_path, 'w', encoding='utf-8') as temp_f:

        idx = 0
        jsonl_iter = iter(jsonl_f)

        for member in tar.getmembers():
            # Skip non-mp3 files
            if not (member.isfile() and member.name.lower().endswith('.mp3')):
                continue

            idx += 1

            # Get the next non-empty line from JSONL
            line = None
            while True:
                try:
                    line = next(jsonl_iter)
                    if line.strip():
                        break
                except StopIteration:
                    raise ValueError(
                        f"More mp3 files in tar than entries in JSONL at member {member.name}"
                    )

            # Try to parse the JSON entry
            try:
                entry = json.loads(line)
            except json.JSONDecodeError as e:
                logging.error(f"[{idx}] Failed to parse JSON for {member.name}: {e}")
                # Write error entry instead of original
                error_entry = {
                    "tar_file": str(tar_path),
                    "audio_file": member.name,
                    "error": f"JSON parse error: {str(e)}"
                }
                temp_f.write(json.dumps(error_entry) + '\n')
                continue

            # Handle entries that already had errors during original processing
            if "error" in entry:
                temp_f.write(json.dumps(entry) + '\n')
                logging.warning(f"[{idx}] Skipping entry with error: {entry.get('audio_file')}")
                continue

            # Verify the audio file name matches
            audio_file_name = entry.get("audio_file", "")
            if audio_file_name != member.name:
                raise ValueError(
                    f"Entry mismatch at index {idx}: "
                    f"JSONL has '{audio_file_name}', tar has '{member.name}'"
                )

            # Skip if entry already has original_metadata
            if "original_metadata" in entry:
                temp_f.write(json.dumps(entry) + '\n')
                logging.debug(f"[{idx}] Skipping {member.name}: already has original_metadata")
                if idx % 100 == 0:
                    logging.info(f"Progress: {idx}")
                continue

            logging.debug(f"[{idx}] Processing: {member.name}")

            try:
                # Extract file and load metadata
                file_obj = tar.extractfile(member)
                if file_obj:
                    _, metadata = load_audio_mono(file_obj)

                    # Create new entry with fields in the same order as tokenize_audio.py
                    updated_entry = {
                        "tar_file": entry.get("tar_file", str(tar_path)),
                        "audio_file": entry["audio_file"],
                        "num_samples": entry["num_samples"],
                        "sample_rate": entry["sample_rate"],
                        "original_metadata": audio_metadata_to_dict(metadata),
                        "tokens": entry["tokens"],
                        "token_shape": entry["token_shape"]
                    }

                    # Write updated entry
                    temp_f.write(json.dumps(updated_entry) + '\n')

                    if idx % 100 == 0:
                        logging.info(f"Progress: {idx}")
                else:
                    raise ValueError(f"Could not extract {member.name} from tar")

            except Exception as ex:
                logging.error(f"[{idx}] Failed to process {member.name}: {ex}", exc_info=True)
                # Write error entry
                error_entry = {
                    "tar_file": str(tar_path),
                    "audio_file": member.name,
                    "error": str(ex)
                }
                temp_f.write(json.dumps(error_entry) + '\n')

        # Check if there are remaining JSONL lines
        for line in jsonl_iter:
            if line.strip():
                raise ValueError(
                    f"More entries in JSONL than mp3 files in tar (processed {idx} members)"
                )

    # If we got here successfully, replace the original file
    logging.info(f"Replacing original file: {jsonl_path}")
    temp_path.replace(jsonl_path)
    logging.info(f"Successfully processed {idx} entries")

    return idx


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Update metadata information in existing JSONL files",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(
        "--jsonl-pattern",
        type=str,
        required=True,
        help="Glob pattern for JSONL files to update (e.g., 'output/*.jsonl')"
    )

    parser.add_argument(
        "--tar-source-dir",
        type=str,
        required=True,
        help="Directory containing source tar files"
    )

    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="List files that would be processed without actually processing them"
    )

    return parser.parse_args()


def main():
    """Main entry point."""
    args = parse_args()

    setup_logging(args.verbose)

    logging.info("=" * 80)
    logging.info("Update Metadata in JSONL Files")
    logging.info("=" * 80)

    # Find matching JSONL files
    jsonl_files = find_matching_jsonl_files(args.jsonl_pattern)

    if not jsonl_files:
        logging.error(f"No JSONL files found matching pattern: {args.jsonl_pattern}")
        sys.exit(1)

    logging.info(f"Found {len(jsonl_files)} JSONL files")

    # Verify tar source directory exists
    tar_source_dir = Path(args.tar_source_dir)
    if not tar_source_dir.exists():
        logging.error(f"Tar source directory not found: {tar_source_dir}")
        sys.exit(1)

    if not tar_source_dir.is_dir():
        logging.error(f"Tar source path is not a directory: {tar_source_dir}")
        sys.exit(1)

    # Process each JSONL file
    total_updated = 0
    failed_files = []

    for jsonl_path in jsonl_files:
        try:
            # Get corresponding tar file
            tar_path = get_tar_path_for_jsonl(jsonl_path, tar_source_dir)

            if args.dry_run:
                logging.info(f"Would process: {jsonl_path}")
                logging.info(f"  Source tar: {tar_path}")
                if not tar_path.exists():
                    logging.warning(f"  WARNING: Tar file not found!")
                continue

            # Update the JSONL file
            num_updated = update_jsonl_with_metadata(
                jsonl_path,
                tar_path
            )

            total_updated += num_updated
            logging.info(f"Successfully updated {num_updated} entries in {jsonl_path.name}")

        except Exception as e:
            logging.error(f"Failed to process {jsonl_path}: {e}", exc_info=True)
            failed_files.append(jsonl_path)

    # Summary
    logging.info("=" * 80)
    if args.dry_run:
        logging.info("Dry run complete!")
        logging.info(f"Would process {len(jsonl_files)} JSONL files")
    else:
        logging.info("Processing complete!")
        logging.info(f"Total JSONL files processed: {len(jsonl_files) - len(failed_files)}")
        logging.info(f"Total entries updated: {total_updated}")
        if failed_files:
            logging.warning(f"Failed files: {len(failed_files)}")
            for failed_file in failed_files:
                logging.warning(f"  - {failed_file}")
    logging.info("=" * 80)

    if failed_files and not args.dry_run:
        sys.exit(1)


if __name__ == "__main__":
    main()
